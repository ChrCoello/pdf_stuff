{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "import fitz  # PyMuPDF\n",
    "from dataclasses import dataclass, field\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_KEY = \"<YOUR_OPEN_AI_KEY>\"  # <YOUR_OPEN_AI_KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/home/chris/data/pdf_stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one PDF to extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "This project  has received  funding  from  the European Union’s \n",
      "Horizon 2020 research and innovation programme under grant \n",
      "agreement No 821105.   \n",
      " \n",
      " \n",
      " \n",
      "Synthesis of the model, \n",
      "selected results,  and \n",
      "scenario assessment  \n",
      "WP9, Task 9.2, D.9.3 \n",
      "31 August  2023 \n",
      "LOW -CARBON SOCIETY: AN ENHANCED MODELLING TOOL FOR  \n",
      "THE TRANSITION TO SUSTAINABILITY (LOCOMOTION)  \n",
      "H2020 -LC-CLA -2018 -2 \n",
      " \n",
      " \n",
      " \n",
      " I / XI \n",
      "SYNTHESIS OF THE MODEL, SELECTED RESULTS, AND SCENARIO ASSESSMENT  \n",
      "DOCUMENT HISTORY  \n",
      " \n",
      "Project Acronym  LOCOMOTION  \n",
      "Project t itle Low-carbon society: an enhanced modelling tool for the transition to \n",
      "sustainability  \n",
      "Project c oordinat ion Universidad de Valladolid (Spain)  \n",
      "Project d uration  1st June 2019 – 30th November  2023 \n",
      "Deliverable No.  D9.3. Synthesis of the model, selected results and scenario assessment  \n",
      "Dissemination level  Confidential (CO)  / Public (PU)  \n",
      "Status   In progress  \n",
      " To be verifi ed by other WPs  \n",
      " Final  \n",
      "Due d ate of deliverable  31st of August  2023 \n",
      "Delivery date  31st of August  2023 \n",
      "Version  v.1.7 \n",
      "Work p ackage  WP9 - Technical coordination and quality assurance  \n",
      "Lead beneficiary  UVa  \n",
      "Author (s)  Mohamed Lifi (UVa), Ignacio de Blas (UVa), Iñigo Capellán -Pérez (UVa), \n",
      "Margarita Mediavilla (UVa), Luis Javier Miguel (UVa), Gonzalo Parrado -\n",
      "Hernado (UVa), Luis LLases (UVa), David Álvarez Antelo (UVa), Marta \n",
      "Calleja (UVa),  Nathalie Wergles  (UVa), Noelia Ferreras (CARTIF), Iván \n",
      "Ramos (CARTIF), Iñaki Arto (BC3 ), Tomás Calheiros (FC.ID), Tiago \n",
      "Capela Lourenço (FC.ID), Guilherme Spinato Morlin (UNIPI), Simone \n",
      "D'Alessandro (UNIPI), Ole van Allen (INN),  Lukas Eggler (AE A), \n",
      "Stavroula Papagianni  (CRES) , Robert Oakes (UNU).  \n",
      " \n",
      "Date  Ver. Contributors  Comment  \n",
      "26/06/2023 0.1 Mohamed Lifi (UVa), Ignacio \n",
      "de Blas (UVa), Luis Javier \n",
      "Miguel (UVa) , Nathalie \n",
      "Wergles  (UVa) .  Table of contents  \n",
      "27/06/2023 0.2 Mohamed Lifi (UVa) & Ignacio \n",
      "de Blas (UVa)  First draft  \n",
      "03/07/2023  0.3 Iñigo Capellán -Pérez (UVa)  & \n",
      "Mohamed Lifi ( UVa) The overview and structure of WILIAM \n",
      "model are added  \n",
      "03/07/2023 0.4 Margarita Mediavilla (UVa) &  \n",
      "Mohamed Lifi ( UVa)  The description and selected results \n",
      "sections of Land and Water Module are \n",
      "added  \n"
     ]
    }
   ],
   "source": [
    "fn = Path(\n",
    "    \"/home/chris/data/pdf_stuff/D.9.3 Synthesis of the model, selected results, and scenario assessment-1.pdf\"\n",
    ")\n",
    "reader = PdfReader(fn)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0:2]\n",
    "\n",
    "text = []\n",
    "# title = extract_title_from_pdf(pdf)\n",
    "for page in reader.pages[0:2]:\n",
    "    text.append(page.extract_text())\n",
    "formatted_txt = \"\".join(text)  # from list to string\n",
    "print(formatted_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Author': 'LOCOMOTION TEAM',\n",
       " '/CreationDate': \"D:20230831112734+02'00'\",\n",
       " '/Creator': 'Microsoft® Word para Microsoft 365',\n",
       " '/Keywords': 'Locomotion,H2020,Deliverable, template',\n",
       " '/ModDate': \"D:20230831112756+02'00'\",\n",
       " '/Producer': 'Microsoft® Word para Microsoft 365',\n",
       " '/Subject': 'Deliverable',\n",
       " '/Title': 'LOCOMOTION-Deliverable'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the metadata\n",
    "reader.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract the title of the PDF using the largest font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Check if PDF is not empty\n",
    "    if len(doc) > 0:\n",
    "        # Get the first page\n",
    "        first_page = doc[0]\n",
    "        # Dictionary to hold text and its font size\n",
    "        text_font_size = {}\n",
    "        # Extract text blocks\n",
    "        for text_block in first_page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if \"lines\" in text_block:  # Ensure it's a text block\n",
    "                for line in text_block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        # Store text and its font size\n",
    "                        text_font_size[span[\"text\"]] = span[\"size\"]\n",
    "        # Find the text with the largest font size\n",
    "        title = max(text_font_size, key=text_font_size.get)\n",
    "        return title\n",
    "    else:\n",
    "        return \"PDF is empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to send the text to ChatGPT and get the title and keywords back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_openai(text):\n",
    "    client = OpenAI(api_key=OPEN_AI_KEY)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a scientific assistant, skilled in detecting titles and keywords in PDF documents, and outputing a JSON object. \\\n",
    "                    For all the content you get try to find the existing title in the document. If you find it in the document, return it. If you don't find it in the document, make up one based only on the content.\\\n",
    "                    In addition, try to find 5 keywords in the document. If you don't find any keywords, return 5 keywords based on the content of the document.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": '{\"title\": \"Defining happiness in Amazonian forest tribes\",  \"keywords\": [\"happiness\", \"social science\",\"primitive socities\",\"anthropology\",\"Sout America\"]}',\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = extract_from_openai(formatted_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"title\": \"Low-carbon society: Synthesis of model and scenario assessment\",  \"keywords\": [\"Horizon 2020\", \"low-carbon society\", \"sustainability\", \"modelling tool\", \"transition\"]}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over all PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDF:\n",
    "    original_pdf_name: str\n",
    "    title_from_metadata: str = \"\"\n",
    "    title_from_openai: str = \"\"\n",
    "    keywords_from_metadata: list = field(default_factory=list)\n",
    "    keywords_from_openai: list = field(default_factory=list)\n",
    "    process_error: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF 1 of 104\n",
      "found metadata\n",
      "Processing PDF 2 of 104\n",
      "found metadata\n",
      "Processing PDF 3 of 104\n",
      "found metadata\n",
      "Processing PDF 4 of 104\n",
      "found metadata\n",
      "Processing PDF 5 of 104\n",
      "found metadata\n",
      "Processing PDF 6 of 104\n",
      "found metadata\n",
      "Processing PDF 7 of 104\n",
      "found metadata\n",
      "Processing PDF 8 of 104\n",
      "found metadata\n",
      "Processing PDF 9 of 104\n",
      "found metadata\n",
      "Processing PDF 10 of 104\n",
      "found metadata\n",
      "Processing PDF 11 of 104\n",
      "found metadata\n",
      "Processing PDF 12 of 104\n",
      "found metadata\n",
      "Processing PDF 13 of 104\n",
      "found metadata\n",
      "Processing PDF 14 of 104\n",
      "found metadata\n",
      "Processing PDF 15 of 104\n",
      "found metadata\n",
      "Processing PDF 16 of 104\n",
      "found metadata\n",
      "Processing PDF 17 of 104\n",
      "found metadata\n",
      "Processing PDF 18 of 104\n",
      "found metadata\n",
      "Processing PDF 19 of 104\n",
      "found metadata\n",
      "Processing PDF 20 of 104\n",
      "found metadata\n",
      "Processing PDF 21 of 104\n",
      "found metadata\n",
      "Processing PDF 22 of 104\n",
      "found metadata\n",
      "Processing PDF 23 of 104\n",
      "found metadata\n",
      "Processing PDF 24 of 104\n",
      "found metadata\n",
      "Processing PDF 25 of 104\n",
      "found metadata\n",
      "Processing PDF 26 of 104\n",
      "found metadata\n",
      "Processing PDF 27 of 104\n",
      "found metadata\n",
      "Processing PDF 28 of 104\n",
      "found metadata\n",
      "Processing PDF 29 of 104\n",
      "found metadata\n",
      "'NoneType' object has no attribute 'keys'\n",
      "Processing PDF 30 of 104\n",
      "found metadata\n",
      "Processing PDF 31 of 104\n",
      "no metadata found\n",
      "Processing PDF 32 of 104\n",
      "found metadata\n",
      "Processing PDF 33 of 104\n",
      "found metadata\n",
      "Processing PDF 34 of 104\n",
      "found metadata\n",
      "Processing PDF 35 of 104\n",
      "found metadata\n",
      "Processing PDF 36 of 104\n",
      "found metadata\n",
      "Processing PDF 37 of 104\n",
      "found metadata\n",
      "Processing PDF 38 of 104\n",
      "found metadata\n",
      "Processing PDF 39 of 104\n",
      "found metadata\n",
      "Processing PDF 40 of 104\n",
      "found metadata\n",
      "Processing PDF 41 of 104\n",
      "found metadata\n",
      "Processing PDF 42 of 104\n",
      "found metadata\n",
      "Processing PDF 43 of 104\n",
      "found metadata\n",
      "Processing PDF 44 of 104\n",
      "found metadata\n",
      "Processing PDF 45 of 104\n",
      "found metadata\n",
      "Processing PDF 46 of 104\n",
      "found metadata\n",
      "Processing PDF 47 of 104\n",
      "found metadata\n",
      "Processing PDF 48 of 104\n",
      "found metadata\n",
      "Processing PDF 49 of 104\n",
      "found metadata\n",
      "Processing PDF 50 of 104\n",
      "found metadata\n",
      "Processing PDF 51 of 104\n",
      "found metadata\n",
      "Processing PDF 52 of 104\n",
      "found metadata\n",
      "Processing PDF 53 of 104\n",
      "found metadata\n",
      "Processing PDF 54 of 104\n",
      "found metadata\n",
      "Processing PDF 55 of 104\n",
      "found metadata\n",
      "Processing PDF 56 of 104\n",
      "found metadata\n",
      "Processing PDF 57 of 104\n",
      "found metadata\n",
      "Processing PDF 58 of 104\n",
      "found metadata\n",
      "Processing PDF 59 of 104\n",
      "found metadata\n",
      "Processing PDF 60 of 104\n",
      "found metadata\n",
      "Processing PDF 61 of 104\n",
      "found metadata\n",
      "Processing PDF 62 of 104\n",
      "found metadata\n",
      "Processing PDF 63 of 104\n",
      "found metadata\n",
      "Processing PDF 64 of 104\n",
      "found metadata\n",
      "Processing PDF 65 of 104\n",
      "found metadata\n",
      "Processing PDF 66 of 104\n",
      "found metadata\n",
      "Processing PDF 67 of 104\n",
      "found metadata\n",
      "Processing PDF 68 of 104\n",
      "found metadata\n",
      "Processing PDF 69 of 104\n",
      "found metadata\n",
      "Processing PDF 70 of 104\n",
      "found metadata\n",
      "Processing PDF 71 of 104\n",
      "found metadata\n",
      "Processing PDF 72 of 104\n",
      "found metadata\n",
      "Processing PDF 73 of 104\n",
      "found metadata\n",
      "Processing PDF 74 of 104\n",
      "found metadata\n",
      "Processing PDF 75 of 104\n",
      "found metadata\n",
      "Processing PDF 76 of 104\n",
      "found metadata\n",
      "Processing PDF 77 of 104\n",
      "found metadata\n",
      "Processing PDF 78 of 104\n",
      "found metadata\n",
      "Processing PDF 79 of 104\n",
      "found metadata\n",
      "Processing PDF 80 of 104\n",
      "found metadata\n",
      "'NoneType' object has no attribute 'keys'\n",
      "Processing PDF 81 of 104\n",
      "found metadata\n",
      "Processing PDF 82 of 104\n",
      "found metadata\n",
      "Processing PDF 83 of 104\n",
      "found metadata\n",
      "Processing PDF 84 of 104\n",
      "found metadata\n",
      "Processing PDF 85 of 104\n",
      "found metadata\n",
      "Processing PDF 86 of 104\n",
      "found metadata\n",
      "Processing PDF 87 of 104\n",
      "found metadata\n",
      "Processing PDF 88 of 104\n",
      "found metadata\n",
      "Processing PDF 89 of 104\n",
      "found metadata\n",
      "Processing PDF 90 of 104\n",
      "found metadata\n",
      "Processing PDF 91 of 104\n",
      "found metadata\n",
      "'NoneType' object has no attribute 'keys'\n",
      "Processing PDF 92 of 104\n",
      "found metadata\n",
      "Processing PDF 93 of 104\n",
      "found metadata\n",
      "Processing PDF 94 of 104\n",
      "found metadata\n",
      "Processing PDF 95 of 104\n",
      "found metadata\n",
      "Processing PDF 96 of 104\n",
      "found metadata\n",
      "Processing PDF 97 of 104\n",
      "found metadata\n",
      "Processing PDF 98 of 104\n",
      "found metadata\n",
      "Processing PDF 99 of 104\n",
      "found metadata\n",
      "Processing PDF 100 of 104\n",
      "found metadata\n",
      "Processing PDF 101 of 104\n",
      "found metadata\n",
      "Processing PDF 102 of 104\n",
      "found metadata\n",
      "Processing PDF 103 of 104\n",
      "found metadata\n",
      "Processing PDF 104 of 104\n",
      "found metadata\n"
     ]
    }
   ],
   "source": [
    "NUM_PAGES_TO_EXTRACT = 2\n",
    "\n",
    "count = 0\n",
    "entity_lst = []\n",
    "\n",
    "lst_pdf = list(data_root.glob(\"*.pdf\"))\n",
    "for idx_pdf, pdf in enumerate(lst_pdf):\n",
    "    print(f\"Processing PDF {idx_pdf + 1} of {len(lst_pdf)}\")\n",
    "    entity = PDF(original_pdf_name=pdf.name)\n",
    "    try:\n",
    "        reader = PdfReader(pdf)\n",
    "        text = []\n",
    "        # Extract the 2 first pages (this can vary, can play with this number)\n",
    "        for page in reader.pages[0:NUM_PAGES_TO_EXTRACT]:\n",
    "            text.append(page.extract_text())\n",
    "        formatted_txt = \"\".join(text)  # from list to string\n",
    "        # Send to OpenAI\n",
    "        message = extract_from_openai(formatted_txt)\n",
    "        #\n",
    "        json_content = json.loads(message.content)\n",
    "        entity.title_from_openai = json_content[\"title\"]\n",
    "        entity.keywords_from_openai = json_content[\"keywords\"]\n",
    "        # Check the metadata\n",
    "        if reader.metadata == {}:\n",
    "            print(\"no metadata found\")\n",
    "        else:\n",
    "            print(\"found metadata\")\n",
    "            if \"/Title\" in reader.metadata.keys():\n",
    "                entity.title_from_metadata = reader.metadata[\"/Title\"]\n",
    "            if \"/Keywords\" in reader.metadata.keys():\n",
    "                entity.keywords_from_metadata = reader.metadata[\n",
    "                    \"/Keywords\"\n",
    "                ]  # .split(\",\")\n",
    "                # entity.keywords_from_metadata = entity.keywords_from_metadata\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        entity.process_error = True\n",
    "    #\n",
    "    entity_lst.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = {\"entities\": [ent.__dict__ for ent in entity_lst]}\n",
    "with open(\"entity_lst.json\", \"w\") as fn:\n",
    "    json.dump(output_json, fn, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitlines = text.splitlines()\n",
    "for idx, line in enumerate(splitlines):  # splitlines:\n",
    "    if (line.split(\" \")[0] == \"KEY\") | (line.split(\" \")[0] == \"KEYWORDS\"):\n",
    "        keyword_line = splitlines[idx] + splitlines[idx + 1]\n",
    "        #\n",
    "        print(keyword_line.replace(\"-\", \"\").split(\":\")[1])\n",
    "        keyword_lst = keyword_line.replace(\"-\", \"\").split(\":\")[1].split(\", \")\n",
    "        keyword_lst = [kl.strip() for kl in keyword_lst]\n",
    "        print(keyword_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-stuff-HuSeG4kd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
